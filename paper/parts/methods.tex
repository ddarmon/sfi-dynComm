\section{Methodology}

\subsection{Community Detection}

In network theory communities are usually defined as group of nodes more densely connected among each other than with the rest of the network. Community detection is a well studied but not yet solved problem, and several different methods and algorithms have been proposed. For a complete review of the subject we refer the reader to (\textbf{ref: Fortunato, Santo. "Community detection in graphs." Physics Reports 486.3 (2010): 75-174.}). In our study we deal with \textit{weighted directed} networks, and we are interested in finding \textit{overlapping} modules, rather than partitions of the network, since people can belong to different social groups: they can be interacting with different groups of people (their college friends, their co-workers, their family, etc), and they can belong to different topical communities (a person can be interested both in cycling and politics and talk about the two topics with different groups of people). Most community detection algorithms developed so far are built to find partitions and only very few to find overlapping communities (\textbf{ref: J. Baumes, M.K. Goldberg, M.S. Krishnamoorthy, M.M. Ismail, N. Preston, Finding communities by clustering a graph into overlapping subgraphs, in: N. Guimaraes, P.T. Isaias (Eds.), IADIS AC, IADIS, 2005, pp. 97–104; Palla, Gergely, et al.
 "Uncovering the overlapping community structure of complex networks in nature and society." Nature 435.7043 (2005): 814-818.; S. Zhang, R.-S. Wang, X.-S. Zhang, Identification of overlapping community structure in complex networks using fuzzy c-means clustering, Physica A 374 (2007) 483–490; Gregory S (2007) An algorithm to find overlapping community structure in networks. In: Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2007).
Berlin, Germany: Springer-Verlag. pp 91–102.; T. Nepusz, A. Petroczi, L. Negyessy, F. Bazso, Fuzzy communities and the concept of bridgeness in complex networks, Phys. Rev. E 77 (1) (2008) 016107; 
Lancichinetti A, Fortunato S, Kertesz J (2009) Detecting the overlapping and hierarchical community structure in complex networks. New J Phys 11: 033015.; 
vans TS, Lambiotte R (2009) Line graphs, link partitions, and overlapping communities. Phys Rev E 80: 016105. 25; Kovacs, Istvan A., et al. (2010) 
Community landscapes: An integrative approach to determine overlapping network module hierarchy, identify key nodes and predict network dynamics. PloS ONE 5: e12528}). 
Among these, even fewer can also deal with directed or weighted networks. Palla et al. clique percolation method can for example take into account these two features, but not both at the same time (\textbf{ref: Palla, Gergely, et al. "Uncovering the overlapping community structure of complex networks in nature and society." Nature 435.7043 (2005): 814-818.}). Lancichinetti et. al recently proposed the first method that is able to deal with all these features and a few more at the same time, called OSLOM (Order Statistics Local Optimization Method) (\textbf{ref: Lancichinetti, A., Radicchi, F., Ramasco, J. J., and Fortunato, S. (2011). Finding statistically significant communities in networks. PloS one, 6(4), e18961.}). Their method "is based on the local optimization of a fitness function expressing the statistical significance of clusters with respect to random fluctuations" (\textbf{taken from their paper, to rephrase}), and it allowed us to detect overlapping communities on our weighted directed networks.

\subsection{The Dataset}

The data consists of the Twitter statuses of 15000 users over a 9 week period (from April 25th to June 25th 2011). The users are embedded in a network collected by performing a breadth-first expansion from a seed user. Once the seed user was chosen, the network was expanded to include his/her followers, only including users considered to be active (users who tweeted at least once per day over the past one hundred tweets). Network collection continued in this fashion by considering the active followers of the active followers of the seed, and so on.
Since one of the kind of communities we want to explore is based on the explicit interactions between users, based on retweets and mentions, we filter the dataset in order to take into account only users who make use of this kind of features in their tweets. We define en event of outgoing information for a given user $u$ as either a mention made by $u$ of another user in the network, or a retweet by another user of one of $u$'s tweets. When we mention someone we are in fact sending him some information, and when we are retweeted it means that the person that retweeted us has received some information from us and is sharing it. Symmetrically, we define an event of incoming information for $u$ as either being mentioned, or retweeting another user. We filtered the network by eliminating all the users that have in their tweeting history less than 9 outgoing information and 9 incoming information events, i.e. less than one event per type per week on average. We then further restricted our analysis to the strong giant connected component of the unweighted directed network built from the filtered set of users and whose link represent a user-follower relationship. In this study the link is directed from the user to the follower because this is the direction in which the information flows. The final network consists of 6917 nodes and 1481131 edges.

\textbf{Show the distribution of the number of mentions / retweets / haghstags / tweets / followers per user ?}

\subsection{Activity-Based Communities and Transfer Entropy}

We can view the behavior of a user $u$ on Twitter as a point process, where at any instant $t$ the user has either emitted a tweet ($X_{t}(u) = 1$) or remained silent ($X_{t}(u) = 0$). This is the view of a users dynamics taken in \cite{ver2012information} and \cite{darmon2013understanding}. Thus, we reduce all of the information generated by a user on Twitter to a time series $\{ X_{t}(u)\}$ where $t$ ranges over the time interval for which we have data (9 weeks in this case). Because status updates are only collected in discrete, 1-second time intervals, it is natural to consider the only the discrete times $t = 1 \text{ s}, 2 \text{ s}, \ldots, $ relative to a reference time. We can then compute the flow of information between two users $u$ and $v$ by computing the transfer entropy between their time series $X_{t}(u)$ and $X_{t}(v).$

Let $\{X_{t}\}$ and $\{ Y_{t}\}$ be two strong-sense stationary stochastic processes. We use the notation $X_{t-k}^{t}$ to denote the values of the stochastic process from time $t-k$ to time $t$, $X_{t-k}^{t} = (X_{t-k}, X_{t-(k-1)}, \ldots, X_{t - 1}, X_{t})$. The lag-$k$ transfer entropy of $Y$ on $X$ is defined as 
\begin{align}
	\text{TE}_{Y \to X}^{(k)} &= H\left[X_{t} | X_{t-k}^{t-1}\right] - H\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right],
\end{align}
where
\begin{align}
	H\left[X_{t} | X_{t-k}^{t-1}\right] = - E\left[ \log_{2} p(X_{t} | X_{t-k}^{t-1}) \right]
\end{align}
and 
\begin{align}
	H\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right] = - E\left[ \log_{2} p(X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}) \right]
\end{align}
are the usual conditional entropies over the conditional (predictive) distributions $p(x_{t} | x_{t-k}^{t-1})$ and $p(x_{t} | x_{t-k}^{t-1}, y_{t-k}^{t-1})$. This formulation was originally developed in~\cite{schreiber2000measuring}, where transfer entropy was proposed as an information theoretic measure of \emph{directed} information flow. Formally, recalling that $H\left[X_{t} | X_{t-k}^{t-1}\right]$ is the uncertainty in $X_{t}$ given its values at the previous $k$ time points, and that $H\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right]$ is the uncertainty in $X_{t}$ given the joint process $\{(X_{t}, Y_{t})\}$ at the previous $k$ time points, transfer entropy measures the reduction in uncertainty of $X_{t}$ by including information about $Y_{t-k}^{t-1},$ controlling for the information in $X_{t - k}^{t-1}$. By the `conditioning reduces entropy' result~\cite{cover2012elements}
\begin{align}
	H[X | Y, Z] \leq H[X | Y],
\end{align}
we can see that transfer entropy is always non-negative, and is zero precisely when $H\left[X_{t} | X_{t-k}^{t-1}\right] = H\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right]$, in which case knowing the past $k$ lags of $Y_{t}$ does not reduce the uncertainty in $X_{t}$. If the transfer entropy is positive, then $\{ Y_{t}\}$ is considered causal for $\{ X_{t}\}$ in the Granger sense~\cite{granger1963economic}.
% More generally we can define the transfer entropy as the limit of lag-$k$ transfer entropies,
% \begin{align}
% 	\text{TE}_{Y \to X} &= \lim_{k \to \infty} \text{TE}_{Y \to X}^{(k)},
% \end{align}
% if the limit exists.

In estimating the transfer entropy from finite data, we will assume that the process $(X_{t}, Y_{t})$ is jointly stationary, which gives us that
\begin{align}
	p(x_{t} | x_{t-k}^{t-1}) = p(x_{k+1} | x_{1}^{k})
\end{align}
and
\begin{align}
	p(x_{t} | x_{t-k}^{t-1}, y_{t-k}^{t-1}) = p(x_{k+1} | x_{1}^{k}, y_{1}^{k})
\end{align}
for all $t$. That is, the predictive distribution only depends on the past, not on when the past is observed\footnote{We really only need \emph{conditional} stationarity~\cite{caires2003nonparametric}, but stationarity implies conditional stationarity}. Given this assumption, we compute estimators for $p(x_{k+1} | x_{1}^{k})$ and $p(x_{k+1} | x_{1}^{k}, y_{1}^{k})$ by `counting': for each possible past $(x_{1}^{k}, y_{1}^{k})$, we count the number of times a future of type $x_{k+1}$ occurs, and normalize. Call these estimators $\hat{p}(x_{k+1} | x_{1}^{k})$ and $\hat{p}(x_{k+1} | x_{1}^{k}, y_{1}^{k})$. Then the plug-in estimator for the transfer entropy is
\begin{align}
	\widehat{\text{TE}}_{Y \to X}^{(k)} &= \hat{H}\left[X_{t} | X_{t-k}^{t-1}\right] - \hat{H}\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right]
\end{align}
where we use the plug-in estimators $\hat{H}\left[X_{t} | X_{t-k}^{t-1}\right]$ and $\hat{H}\left[X_{t} | X_{t-k}^{t-1}, Y_{t-k}^{t-1}\right]$ for the entropies. It is well known that the plug-in estimator for entropy is biased~\cite{paninski2003estimation}. To account for this bias, we use the Miller-Madow adjustment to the plug-in estimator~\cite{miller1955note}.

For the communities based on transfer entropy, we weight each edge from a user $u$ to a follower $f$ by the estimated transfer entropy of the user $u$ on $f$, 
\begin{align}
	w_{u \to f}^{\text{TE}(k)} = \widehat{\text{TE}}_{X(u) \to X(f)}^{(k)}.
\end{align}

\section{Interaction-Based Communities and Mention / Retweet Weighting}
A way of tracking the flow of information through users is by considering two useful features of this social network: mentions and retweets. Through mentions users can in fact send a direct message to other users. And a retweet means that a piece of information from a user has been captured by a follower and shared with his/her own followers. We can therefore define interaction-based communities by weighting the user-follower network with a measure proportional to the number of mentions and retweets between users. In more details, for each couple of user-follower $u$ and $f$, we use the arithmetic / harmonic (\textbf{which one?}) mean of the two following numbers:
\begin{equation}
\frac{\mbox{\# mentions of }f \mbox{ by }u}{\mbox{\# total mentions of }f}
\end{equation}
and
\begin{equation}
\frac{\mbox{\# retweets of }u \mbox{ by }f}{\mbox{\# total retweets made by }f} \mbox{ .}
\end{equation}
For the communities based on mention-retweets, we weight each edge from a user $u$ to a follower $f$ by 
\begin{align}
	w_{u \to f}^{\text{MR}} = \frac{1}{2}\left(\frac{\mbox{\# mentions of }f \mbox{ by }u}{\mbox{\# total mentions of }f} + \frac{\mbox{\# retweets of }u \mbox{ by }f}{\mbox{\# total retweets made by }f}\right).
\end{align} 

\section{Topic-based Communities and Hashtag Weighting}

Another kind of community is the one based on the content of the tweets, and relies on the idea of finding people that talk about the same things. In order to detect this kind of communities, we weight the edges of the user-follower network through a measure based on the number of common hashtags between the two users. Hashtags are in fact a good proxy for this, since they are explicitly meant to be key-words indicating a particular topic. Moreover they are widely used and straightforwardly detectable. We characterize each user $u$ by a vector $\vec{h}(u)$ of length equal to the number of hashtags in the dataset, and whose elements are defined as
\begin{equation}
h_i(u) = n_i(u) * \log{ \frac{N}{n_i} }
\end{equation}
where $n_i(u)$ is the frequency of the hashtag $i$ in the set of user $u$'s tweets, $N$ is the total number of users, and $n_i$ is the number of users that have used the hashtag $i$ in their tweets. This adapted term frequency--inverse document frequency (tf-idf) measure (\textbf{ref: Salton, Gerard, and Michael J. McGill. "Introduction to modern information retrieval." (1986).}) captures the importance of a hashtag in the users's tweets through the first factor, but at the same time smooths it through the second factor by giving less importance to hashtags that are too widely used (as $\frac{N}{n_i}$ approaches one, its logarithm approaches zero). For each couple of user linked in the user-follower network, we then compute the cosine similarity of their respective vectors, and assign the obtained value as the weight of the directed edge(s) connecting them. This weight captures the similarity between users in terms of topic discussed in their tweets. Thus, for communities based on hashtag similarity, we weight each edge from a user $u$ to a follower $f$ by 
\begin{align}
	w_{u \to f}^{\text{HT}} = \frac{\vec{h}(u) \cdot \vec{h}(f)}{||\vec{h}(u)|| \ \ ||\vec{h}(f)||}.
\end{align}